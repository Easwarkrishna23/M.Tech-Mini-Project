# M.Tech-Mini-Project
Predictor for Multiple Cardiac Risks: Leverages Multi-Layer Perceptron (MLP) models trained on synthetic multimodal patient data (ECG, EEG, MRI, Wearable) to assess risk for Arrhythmia, CAD, Heart Failure, and Hypertension. Features an interactive Streamlit web app with SHAP interpretability.


# Multimodal Cardiac Risk Prediction using MLP and SHAP

This project demonstrates building and deploying machine learning models to predict the risk of multiple cardiac conditions (Arrhythmia, Coronary Artery Disease, Heart Failure, Hypertension) based on synthetic multimodal patient data, including demographics, ECG, EEG, MRI, and wearable sensor data. The project uses PyTorch for model training and a Streamlit web application for interactive risk prediction and model interpretability using SHAP values.

## Features

*   **Synthetic Data Generation:** Includes a script to generate realistic synthetic patient data with various features and multiple binary target variables representing different cardiac risks.
*   **Multimodal Data Processing:** Loads and preprocesses tabular data derived from different simulated modalities.
*   **Multiple Model Training:** Trains separate Multi-Layer Perceptron (MLP) binary classification models for each specific cardiac risk target.
*   **Model Evaluation:** Evaluates trained models using standard classification metrics (Accuracy, Precision, Recall, F1-Score, Confusion Matrix) and visualizes training history (Loss and Accuracy curves).
*   **Model & Scaler Persistence:** Saves the trained models and the data scaler for later use in the web application.
*   **Interactive Web UI (Streamlit):** Provides a user-friendly web interface to input patient data and get real-time risk predictions for each cardiac condition.
*   **Interpretable Predictions (SHAP):** Integrates SHAP (SHapley Additive exPlanations) to visualize the contribution of each input feature to the model's prediction for a specific patient.
*   **Metric Explanation:** Includes a simple explanation of key classification metrics for non-technical users within the UI.

## Technologies Used

*   Python
*   PyTorch
*   Pandas
*   NumPy
*   Scikit-learn
*   Matplotlib
*   Streamlit
*   SHAP

## Setup and Installation

1.  **Clone the repository:**
    ```bash
    git clone <repository_url>
    cd <repository_name>
    ```
2.  **Create a virtual environment (recommended):**
    ```bash
    python -m venv .venv
    # On macOS/Linux:
    source .venv/bin/activate
    # On Windows:
    .venv\Scripts\activate
    ```
3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    *(Note: You will need to create a `requirements.txt` file containing the libraries listed under "Technologies Used". You can generate one automatically after installing them using `pip freeze > requirements.txt`)*
4.  **Create necessary directories:**
    ```bash
    mkdir saved_models saved_scaler
    ```

## How to Run

1.  **Generate Data and Train Models:**
    Run the main training script. This script will:
    *   Generate the synthetic dataset (`full_synthetic_cardio_neuro_data.csv`).
    *   Split the data into training (`train_data.csv`) and testing (`test_data.csv`) sets with scaled features.
    *   Save the feature scaler (`saved_scaler/scaler.pkl`).
    *   Train separate MLP models for each cardiac target and save them (`saved_models/mlp_<target_name>.pth`).
    *   Display training history plots (close these windows to continue).
    *   Print final evaluation metrics for each model.

    ```bash
    python cardiac_risk_predictor.py
    ```

2.  **Run the Streamlit Web Application:**
    Once the models and scaler are saved, start the Streamlit application.

    ```bash
    streamlit run streamlit_app.py
    ```
    This will open the application in your default web browser (usually at `http://localhost:8501`).

## Project Files

*   `synthetic_data_generator.py`: Script to generate the initial synthetic dataset.
*   `cardiac_risk_predictor.py`: Contains the core logic for data loading, preprocessing, splitting, training, evaluation, plotting, and saving models/scaler for multiple cardiac targets.
*   `streamlit_app.py`: The Streamlit application code for the interactive UI, prediction inference using saved models, and SHAP visualization.
*   `full_synthetic_cardio_neuro_data.csv`: The full generated dataset (created by `cardiac_risk_predictor.py` if not present).
*   `train_data.csv`: The training dataset with scaled features and all targets (generated by `cardiac_risk_predictor.py`).
*   `test_data.csv`: The testing dataset with scaled features and all targets (generated by `cardiac_risk_predictor.py`).
*   `saved_models/`: Directory containing the saved PyTorch model files (`.pth`), one for each cardiac target.
*   `saved_scaler/`: Directory containing the saved StandardScaler object (`scaler.pkl`).
*   `requirements.txt`: List of Python dependencies.

## Data

The project utilizes a synthetic dataset designed to simulate multimodal patient data relevant to cardiovascular and neurological health. It includes features such as age, gender, medical history flags, medication count, various ECG measurements, EEG power metrics, MRI findings, and wearable device data (steps, sleep). The dataset includes binary targets for Arrhythmia, CAD, Heart Failure, and Hypertension risks, along with Alzheimer's risk. The correlations in the synthetic data are artificial and intended solely for demonstration purposes.

## Model

A simple Multi-Layer Perceptron (MLP) neural network is used for classification. A separate MLP model is trained independently for each specific cardiac risk target (`target_arrhythmia`, `target_cad`, `target_heart_failure`, `target_hypertension`), treating each as a binary classification problem.

## Interpretability (SHAP)

SHAP (SHapley Additive exPlanations) values are employed to provide local interpretability for individual predictions made by the models in the Streamlit application. The SHAP force plot visualizes how each feature contributes to pushing the prediction probability away from the average prediction towards the specific prediction for the entered patient data.

## Understanding Performance Metrics

The `cardiac_risk_predictor.py` script outputs several key classification metrics to evaluate the trained models:

*   **Accuracy:** Overall proportion of correct predictions.
*   **Precision:** Ability of the model to not predict a positive outcome when it's actually negative (minimizing False Positives).
*   **Recall (Sensitivity):** Ability of the model to find all the actual positive outcomes (minimizing False Negatives).
*   **F1-Score:** The harmonic mean of Precision and Recall, providing a balance between the two.
*   **Confusion Matrix:** A table summarizing prediction results (True Positives, True Negatives, False Positives, False Negatives).

These metrics help assess the model's performance beyond just simple accuracy, which is particularly important in medical applications where the cost of False Positives and False Negatives can differ significantly. The Streamlit app includes a brief explanation of these terms for clarity.

## Example Input

You can try entering data for a simulated patient to see the predictions. For instance, consider a high-risk profile:

*   **Age:** 75
*   **Gender:** Male (1)
*   **History Hypertension:** Yes (1)
*   **History Diabetes:** Yes (1)
*   **Num Medications:** 8
*   **ECG HR:** 95
*   **ECG HRV SDNN:** 25.0
*   **ECG QTc:** 450
*   **ECG Premature Beats:** Yes (1)
*   **EEG Alpha Power:** 15.0
*   **EEG Theta Delta Ratio:** 2.5
*   **EEG Spike Wave:** No (0)
*   **MRI Hippocampal Volume:** 3.000
*   **MRI WMH Score:** 9.0
*   **Wearable Avg Steps:** 2000
*   **Wearable Avg Sleep Hrs:** 4.0

Entering these values in the Streamlit UI and clicking "Predict" should likely result in higher predicted risks for the cardiac conditions, and the SHAP plots will show which of these input values contributed most significantly to those predictions.

*(You can add a screenshot of your Streamlit app here once you have it running!)*

## Future Enhancements (Ideas)

*   Using real-world, anonymized medical datasets (with appropriate permissions and ethical considerations).
*   Exploring more advanced model architectures (e.g., incorporating modality-specific encoders, attention mechanisms, multi-task learning).
*   Implementing a more robust validation strategy (e.g., K-fold cross-validation).
*   Adding more sophisticated data augmentation techniques for synthetic data.
*   Including confidence scores or uncertainty estimates with predictions.
*   Improving the Streamlit UI with more detailed visualizations or interactive features.
*   Deploying the application on a cloud platform (e.g., Heroku, AWS, Google Cloud).

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details. *(Remember to add a LICENSE file to your repository)*
